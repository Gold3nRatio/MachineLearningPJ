str(b)
names(b)
head(b$classe)
head(data$classe)
c(-2:-5)
dt <- data[, c(-2:-5, -160)]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
c <- cbind(b, factor(data$classe))
str(c)
data.frame(head(data$min_yaw_dumbbell, 30), head(a$min_yaw_dumbbell, 30), head(b$min_yaw_dumbbell, 30))
data.frame(data=head(data$min_yaw_dumbbell, 30)
, a=head(a$min_yaw_dumbbell, 30)
, b=head(b$min_yaw_dumbbell, 30)
, c=head(c$min_yaw_dumbbell, 30))
data.frame(data=head(data$classe, 30)
, a=head(a$classe, 30)
, b=head(b$classe, 30)
, c=head(c$classe, 30))
data.frame(data=head(data$classe, 30)
#            , a=head(a$classe, 30)
#            , b=head(b$classe, 30)
, c=head(c$classe, 30))
c <- cbind(b, classe=factor(data$classe))
data.frame(data=head(data$classe, 30)
#            , a=head(a$classe, 30)
#            , b=head(b$classe, 30)
, c=head(c$classe, 30))
rm(a,b)
rm(c)
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
data <- read.csv(paste0(fileLoc, 'pml-training.csv'))
# see the structure of the data
str(data)
# remove some variables (timestamps) not needed, assume work out is independent
# of time performed, then change the variables to numeric.  Finally, add the
# classe variable back in
dt <- data[, c(-2:-5, -160)]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
dt <- cbind(b, classe=factor(data$classe))
# remove the not needed variables
rm(a,b)
str(dt)
# load the require libraries
library(caret)
library(randomForest)
# partition the data into training and testing set for cross-validatoin
set.seed(11235)
inTrain = createDataPartition(data$classe, p = 3/4)[[1]]
training = data[ inTrain,]
testing = data[-inTrain,]
# size of each
dim(training)
dim(testing)
# train the dataset to find the importance of variables using Random Forest
vfit <- randomForest(classe ~ ., data=training, importance=TRUE, na.action=na.omit)
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
dim(training)
dim(testing)
vfit <- randomForest(classe ~ ., data=training, importance=TRUE, na.action=na.omit)
summary(dt$classe)
summary(training$classe)
names(training)
str(dt$classe)
table(dt$classe)
dt <- data[, c(-2:-5, -160)]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
dt <- cbind(b, classe=data$classe)
table(dt$classe)
vfit <- randomForest(classe ~ ., data=training, importance=TRUE, na.action=na.omit)
dt <- cbind(b, classe=as.factor(data$classe))
dt <- cbind(b, classe=data$classe)
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
vfit <- randomForest(classe ~ ., data=training, importance=TRUE, na.action=na.omit)
dt <- cbind(b, classe=as.factor(data$classe))
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
vfit <- randomForest(classe ~ ., data=training, importance=TRUE, na.action=na.omit)
mrf <- train(classe ~ ., data=training, importance=TRUE)
dim(dt)
complete.cases(dt)
dim(dt)
sum(complete.cases(dt))
View(data)
sum(complete.cases(data))
sum(complete.cases(data,2))
colSums(is.na(data))
View(data)
colSums(is.na(data)>1)
a <- colSums(is.na(data))
str(a)
a <- colSums(is.na(data)) > 0
str(a)
a
a[,1]
str(a)
names(a)
a <- data.frame(colSums(is.na(data)))
a
a[,1]
data.frame(nacount = colSums(is.na(data)))
data.frame(nacount = colSums(is.na(data))) -> a
row.names(a)
row.names(a)[1]
dim(data)
data.frame(nacount = colSums(is.na(data)))
dim(data)
str(nadf)
nadf <- data.frame(nacount = colSums(is.na(data)))
str(nadf)
nadf$nacount > nrows(data)/2
nrow(data)
nadf$nacount > nrow(data)/2
row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
alist <- c('a', 'b', 'c')
adf <- data.frame(a=c(1:10), b=a*2, c=a+b, d=a*b-c)
adf <- data.frame(a=c(1:10), b=c(1:10), c=c(1:10), d=c(1:10))
adf
alist
adf[names(adf) %in% alist]
adf[names(adf) %not in% alist]
adf[names(adf) %in% -alist]
adf[!names(adf) %in% alist]
str(data)
dim(data)
nadf <- data.frame(nacount = colSums(is.na(data)))
nadf
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
str(dt)
ncol(dt)
names(dt)
dt <- data[, c(-2:-5, -(ncol(dt)))]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
dt <- cbind(b, classe=as.factor(data$classe))
str(dt)
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
dt <- data[, c(-2:-5, -(ncol(dt)))]
str(dt)
ncol(dt)
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
ncol(dt)
dt <- dt[, c(-2:-5, -(ncol(dt)))]
ncol(dt)
str(dt)
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
str(a)
str(b)
dt <- dt[, c(-2:-6, -(ncol(dt)))]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
str(a)
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
dt <- dt[, c(-2:-6, -(ncol(dt)))]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
str(a)
names(a)
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
# remove some variables (timestamps) not needed, assume work out is independent
# of time performed, then change the variables to numeric.  Finally, add the
# classe variable back in
dt <- dt[, c(-2:-7, -(ncol(dt)))]
str(dt)
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
str(b)
colSums(is.na(b))
data.frame(colSums(is.na(b)))
nadf <- data.frame(nacount = colSums(is.na(b)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
b <- b[!names(b) %in% nacol]
str(b)
complete.cases(b)
sum(complete.cases(b))
dim(b)
dt <- cbind(b, classe=data$classe)
str(dt)
ls()
rm(!list = c(fileLoc, data, dt))
rm(!list %in% c(fileLoc, data, dt))
rm(!list =  c('fileLoc', 'data', 'dt'))
rm(!list %in%  c('fileLoc', 'data', 'dt'))
rm(list !=  c('fileLoc', 'data', 'dt'))
rm(list =  c('fileLoc', 'data', 'dt'))
rm(list=ls())
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
data <- read.csv(paste0(fileLoc, 'pml-training.csv'))
# see the structure of the data
str(data)
dim(data)
# find columns with all NA's and remove
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
# remove some variables (timestamps) not needed, assume work out is independent
# of time performed, then change the variables to numeric.  Finally, add the
# classe variable back in
dt <- dt[, c(-2:-7, -(ncol(dt)))]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
nadf <- data.frame(nacount = colSums(is.na(b)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
b <- b[!names(b) %in% nacol]
dt <- cbind(b, classe=data$classe)
rm(list =  -c('fileLoc', 'data', 'dt'))
rm(list =  !c('fileLoc', 'data', 'dt'))
rm(!list = c('fileLoc', 'data', 'dt'))
rm(!list == c('fileLoc', 'data', 'dt'))
rm(list =setdiff(ls(), c('fileLoc', 'data', 'dt')))
ls()
str(dt)
library(caret)
library(randomForest)
# partition the data into training and testing set for cross-validatoin
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
# size of each
dim(training)
dim(testing)
vfit <- randomForest(classe ~ ., data=training, importance=TRUE)
mrf <- train(classe ~ ., data=training, importance=TRUE)
varImpPlot(vfit)
importance(vfit)
set.seed(4543)
data(mtcars)
mtcars.rf <- randomForest(mpg ~ ., data=mtcars, ntree=1000, keep.forest=FALSE,
importance=TRUE)
varImpPlot(mtcars.rf)
importance(mtcars.rf)
# find columns with all NA's and remove
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
# remove some variables (timestamps) not needed, assume work out is independent
# of time performed, then change the variables to numeric.  Finally, add the
# classe variable back in
dt <- dt[, c(-1:-7, -(ncol(dt)))]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
nadf <- data.frame(nacount = colSums(is.na(b)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
b <- b[!names(b) %in% nacol]
dt <- cbind(b, classe=data$classe)
# remove the not needed variables
rm(list =setdiff(ls(), c('fileLoc', 'data', 'dt')))
# load the require libraries
library(caret)
library(randomForest)
# partition the data into training and testing set for cross-validatoin
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
# size of each
dim(training)
dim(testing)
# train the dataset to find the importance of variables using Random Forest
vfit <- randomForest(classe ~ ., data=training, importance=TRUE)
#mrf <- train(classe ~ ., data=training, importance=TRUE)
importance(vfit)
str(dt)
vfit
plot(vfit)
rm(list =setdiff(ls(), c('fileLoc', 'data', 'dt')))
# partition the data into training and testing set for cross-validatoin
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
# size of each
dim(training)
dim(testing)
# train the dataset to find the importance of variables using Random Forest
rffit <- randomForest(classe ~ ., data=training, importance=FALSE)
varImpPlot(vfit)
varImpPlot(RFfit)
varImpPlot(rffit)
importance(rffit)
rffit
pre_test <- predict(rffit, data=testing)
pre_test
sum(testing$classe == pre_test$classe) / length(testing$classe)
sum(testing$classe == pre_test$classe) / nrow(testing$classe)
dim(testing$classe)
dim(testing)
str(testing)
head(testing$classe)
head(pre_test$classe)
head(pre_test)
sum(testing$classe == pre_test) / nrow(testing$classe)
sum(testing$classe == pre_test) / length(testing$classe)
sum(testing$classe == pre_test) / length(testing)
length(testing$classe)
length(pre_test)
dim(testing$classe)
testing$classe
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
str(AlzheimerDisease)
head(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
str(AlzheimerDisease)
dim(pre_test)
length(pre_test)
length(testing$classe)
dim(testing)
dim(training)
pre_test <- predict(rffit, data=testing)
length(pre_test)
rffit <- randomForest(classe ~ ., data=training)
rffit
pre_test <- predict(rffit, data=testing)
length(pre_test)
pre_test
rffit <- train(classe ~ ., data=training, method='rf')
rffit <- randomForest(classe ~ ., data=training)
rffit
str(testing)
pre_test <- predict(rffit, data=testing(,ncol(testing)))
length(pre_test)
pre_test <- predict(rffit, data=testing(,-ncol(testing)))
dim(testing)
dim(pre_test)
length(pre_test)
str(testing(,-ncol(testing)))
ncol9testing)
ncol(testing)
testing(,ncol(testing))
resting(,1)
testing(,1)
testing[,1]
pre_test <- predict(rffit, data=testing[,-ncol(testing)])
length(pre_test)
pre_test <- predict(rffit, testing[,-ncol(testing)])
length(pre_test)
sum(testing$classe == pre_test) / length(testing)
pre_test <- predict(rffit, testing)
sum(testing$classe == pre_test) / length(testing)
rffit
rf2 <- randomForest(classe ~ ., data=training, ntree=1000)
rf2
rf3 <- randomForest(classe ~ ., data=training, mtry=14)
rf1 <- randomForest(classe ~ ., data=training)
pre1 <- predict(rf1, testing)
pre2 <- predict(rf2, testing)
pre3 <- predict(rf3, testing)
data.frame(pre1 = sum(testing$classe == pre1) / length(testing)
, pre2 = sum(testing$classe == pre2) / length(testing)
, pre3 = sum(testing$classe == pre3) / length(testing))
rf2 <- randomForest(classe ~ ., data=training, ntree=100)
pre2 <- predict(rf2, testing)
data.frame(pre1 = sum(testing$classe == pre1) / length(testing)
, pre2 = sum(testing$classe == pre2) / length(testing)
, pre3 = sum(testing$classe == pre3) / length(testing))
set.seed(647)
myiris <- cbind(iris[1:4], matrix(runif(96 * nrow(iris)), nrow(iris), 96))
result <- rfcv(myiris, iris$Species, cv.fold=3)
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))
result
source('~/Documents/Codes/machLearnPJ/machLearn.R', echo=TRUE)
rf1
rf2
rf
rf3
pwd()
pwd
dir
cd
getwd()
rm(rf1)
rf1
load('wgtlifeRF.data')
rf1
rf1 <- randomForest(classe ~ ., data=training)
save(rf1, file='/home/eric/Documents/Codes/wgtliftRF.rdata')
rm(list=ls())
## change the working directory
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
setwd(fileLoc)
load('wgtlifeRF.data')
getwd()
list.files()
load('wgtlifeRF.rdata')
load('wgtliftRF.rdata')
data <- read.csv(paste0(fileLoc, 'pml-testing.csv'))
dim(data)
rf1 <- load('wgtliftRF.rdata')
ls()
rf1
print(rf1)
str(rf1)
load("/home/eric/Documents/Codes/machLearnPJ/wgtliftRF.rdata")
getwd()
rf1 <- load(paste0(fileLoc,'wgtliftRF.rdata'))
print(rf1)
rf1
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
rm(list=ls())
source('~/Documents/Codes/machLearnPJ/machLearn.R', echo=TRUE)
rm(list=ls())
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
setwd(fileLoc)
rf1 <- load(paste0(fileLoc,'wgtliftRF.rdata'))
load("/home/eric/Documents/Codes/machLearnPJ/wgtliftRF.rdata")
rm(list=ls())
## change the working directory
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
setwd(fileLoc)
load(paste0(fileLoc,'wgtliftRF.rdata'))
## read the testing files
data <- read.csv(paste0(fileLoc, 'pml-testing.csv'))
pre1 <- predict(rf1, data)
pre1
rf1
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0(fileLoc, 'output/', "problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(pre1)
rm(list=ls())
## Machine Learning Project
## This project's goal is to predict how each participant perform the excercise.
fileLoc <- '/home/eric/Documents/Codes/machLearnPJ/'
data <- read.csv(paste0(fileLoc, 'data/pml-training.csv'))
# see the structure of the data
str(data)
dim(data)
# find columns with all NA's and remove
nadf <- data.frame(nacount = colSums(is.na(data)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
dt <- data[!names(data) %in% nacol]
# remove some variables (timestamps) not needed, assume work out is independent
# of time performed, then change the variables to numeric.  Finally, add the
# classe variable back in
dt <- dt[, c(-1:-7, -(ncol(dt)))]
a <- data.frame(lapply(dt, as.character), stringsAsFactors=FALSE)
b <- data.frame(lapply(a, as.numeric))
nadf <- data.frame(nacount = colSums(is.na(b)))
nacol <- row.names(nadf)[c(nadf$nacount > nrow(data)/2)]
b <- b[!names(b) %in% nacol]
dt <- cbind(b, classe=data$classe)
# remove the not needed variables
rm(list =setdiff(ls(), c('fileLoc', 'data', 'dt')))
# load the require libraries
library(caret)
library(randomForest)
# partition the data into training and testing set for cross-validatoin
set.seed(11235)
inTrain = createDataPartition(dt$classe, p = 3/4)[[1]]
training = dt[ inTrain,]
testing = dt[-inTrain,]
# size of each
dim(training)
dim(testing)
rf1 <- randomForest(classe ~ ., data=training, importance=TRUE)
importance(rf1)
vrImp <- importance(rf1)
str(vrImp)
vrImp
vrImp[,1]
vrImp[,2]
dim(vrImp)
vrImp <- data.frame(importance(rf1))
dim(vrImp)
vrImp
View(vrImp)
?order
sort(vrImp$MeanDecreaseAccuracy)
sort(vrImp$MeanDecreaseAccuracy, decreasing=TRUE)
vrImp[sort(vrImp$MeanDecreaseAccuracy, decreasing=TRUE, index.return=TRUE)
sort(vrImp$MeanDecreaseAccuracy, decreasing=TRUE, index.return=TRUE)
vrImp[sort(vrImp$MeanDecreaseAccuracy, decreasing=TRUE, index.return=TRUE),]
a <- sort(vrImp$MeanDecreaseAccuracy, decreasing=TRUE, index.return=TRUE)
a[,1]
str(a)
a$ix
vrImp[sl$ix,]
sl <- sort(vrImp$MeanDecreaseAccuracy, decreasing=TRUE, index.return=TRUE)
vrImp[sl$ix,]
vrImp[sl$ix,]
vrImp[sl$ix,1]
row.names(vrImp[sl$ix,])
head(row.names(vrImp[sl$ix,]))
